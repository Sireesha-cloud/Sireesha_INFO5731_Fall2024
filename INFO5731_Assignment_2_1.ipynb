{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sireesha-cloud/Sireesha_INFO5731_Fall2024/blob/main/INFO5731_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwLvqpqS7l74",
        "outputId": "20c0d866-f70c-488f-da3f-83ab61048aa8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Scraper API Configuration\n",
        "SCRAPER_API_KEY = '06de0be700dfe414381d19c077edc7ac'  # Replace with your Scraper API key\n",
        "BASE_URL = 'http://api.scraperapi.com'\n",
        "\n",
        "# Amazon Product URL (You can change this to any product URL)\n",
        "amazon_product_url = \"https://www.amazon.com/SAMSUNG-Smartphone-Unlocked-Android-Titanium/dp/B0CMDM65JH\"  # Example URL for a product\n",
        "\n",
        "# Function to get Amazon reviews via Scraper API\n",
        "def get_reviews(page):\n",
        "    params = {\n",
        "        'api_key': SCRAPER_API_KEY,\n",
        "        'url': f'{amazon_product_url}?pageNumber={page}'\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Error fetching page {page}: Status Code {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to parse reviews from HTML response\n",
        "def parse_reviews(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    reviews = []\n",
        "\n",
        "    # Select the review blocks (this selector may need to be adjusted based on the actual HTML structure)\n",
        "    review_blocks = soup.select('.review')\n",
        "\n",
        "    for block in review_blocks:\n",
        "        try:\n",
        "            rating = block.select_one('.review-rating').text.strip()\n",
        "            title = block.select_one('.review-title').text.strip()\n",
        "            content = block.select_one('.review-text-content').text.strip()\n",
        "            date = block.select_one('.review-date').text.strip()\n",
        "            reviews.append({\n",
        "                'Rating': rating,\n",
        "                'Title': title,\n",
        "                'Content': content,\n",
        "                'Date': date\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting review: {e}\")\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# Function to save reviews to CSV file\n",
        "def save_reviews_to_csv(reviews, output_file='amazon_reviews_1000.csv'):\n",
        "    if reviews:\n",
        "        keys = reviews[0].keys()\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(reviews)\n",
        "\n",
        "# Main function to collect reviews\n",
        "def collect_amazon_reviews():\n",
        "    all_reviews = []\n",
        "    page = 1\n",
        "    while len(all_reviews) < 1000:  # Collect at least 1000 reviews\n",
        "        print(f\"Fetching reviews from page {page}...\")\n",
        "        html_content = get_reviews(page)\n",
        "\n",
        "        if html_content:\n",
        "            reviews = parse_reviews(html_content)\n",
        "            all_reviews.extend(reviews)\n",
        "            page += 1\n",
        "            time.sleep(2)  # Adding delay between requests to avoid getting blocked\n",
        "        else:\n",
        "            break  # Stop if there are issues fetching pages\n",
        "\n",
        "    print(f\"Collected {len(all_reviews)} reviews.\")\n",
        "    save_reviews_to_csv(all_reviews[:1000])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    collect_amazon_reviews()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHbxLRo27qyk",
        "outputId": "89adb037-e943-4c8f-8e81-98c751eb3d0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching reviews from page 1...\n",
            "Fetching reviews from page 2...\n",
            "Fetching reviews from page 3...\n",
            "Fetching reviews from page 4...\n",
            "Fetching reviews from page 5...\n",
            "Fetching reviews from page 6...\n",
            "Fetching reviews from page 7...\n",
            "Fetching reviews from page 8...\n",
            "Fetching reviews from page 9...\n",
            "Fetching reviews from page 10...\n",
            "Fetching reviews from page 11...\n",
            "Fetching reviews from page 12...\n",
            "Fetching reviews from page 13...\n",
            "Fetching reviews from page 14...\n",
            "Fetching reviews from page 15...\n",
            "Fetching reviews from page 16...\n",
            "Fetching reviews from page 17...\n",
            "Fetching reviews from page 18...\n",
            "Fetching reviews from page 19...\n",
            "Fetching reviews from page 20...\n",
            "Fetching reviews from page 21...\n",
            "Fetching reviews from page 22...\n",
            "Fetching reviews from page 23...\n",
            "Fetching reviews from page 24...\n",
            "Fetching reviews from page 25...\n",
            "Fetching reviews from page 26...\n",
            "Fetching reviews from page 27...\n",
            "Fetching reviews from page 28...\n",
            "Fetching reviews from page 29...\n",
            "Fetching reviews from page 30...\n",
            "Fetching reviews from page 31...\n",
            "Fetching reviews from page 32...\n",
            "Fetching reviews from page 33...\n",
            "Fetching reviews from page 34...\n",
            "Fetching reviews from page 35...\n",
            "Fetching reviews from page 36...\n",
            "Fetching reviews from page 37...\n",
            "Fetching reviews from page 38...\n",
            "Fetching reviews from page 39...\n",
            "Fetching reviews from page 40...\n",
            "Fetching reviews from page 41...\n",
            "Fetching reviews from page 42...\n",
            "Fetching reviews from page 43...\n",
            "Fetching reviews from page 44...\n",
            "Fetching reviews from page 45...\n",
            "Fetching reviews from page 46...\n",
            "Fetching reviews from page 47...\n",
            "Fetching reviews from page 48...\n",
            "Fetching reviews from page 49...\n",
            "Fetching reviews from page 50...\n",
            "Fetching reviews from page 51...\n",
            "Fetching reviews from page 52...\n",
            "Fetching reviews from page 53...\n",
            "Fetching reviews from page 54...\n",
            "Fetching reviews from page 55...\n",
            "Fetching reviews from page 56...\n",
            "Fetching reviews from page 57...\n",
            "Fetching reviews from page 58...\n",
            "Fetching reviews from page 59...\n",
            "Fetching reviews from page 60...\n",
            "Fetching reviews from page 61...\n",
            "Fetching reviews from page 62...\n",
            "Fetching reviews from page 63...\n",
            "Fetching reviews from page 64...\n",
            "Fetching reviews from page 65...\n",
            "Fetching reviews from page 66...\n",
            "Fetching reviews from page 67...\n",
            "Fetching reviews from page 68...\n",
            "Fetching reviews from page 69...\n",
            "Fetching reviews from page 70...\n",
            "Fetching reviews from page 71...\n",
            "Fetching reviews from page 72...\n",
            "Fetching reviews from page 73...\n",
            "Fetching reviews from page 74...\n",
            "Fetching reviews from page 75...\n",
            "Fetching reviews from page 76...\n",
            "Fetching reviews from page 77...\n",
            "Fetching reviews from page 78...\n",
            "Fetching reviews from page 79...\n",
            "Fetching reviews from page 80...\n",
            "Fetching reviews from page 81...\n",
            "Fetching reviews from page 82...\n",
            "Fetching reviews from page 83...\n",
            "Fetching reviews from page 84...\n",
            "Fetching reviews from page 85...\n",
            "Fetching reviews from page 86...\n",
            "Fetching reviews from page 87...\n",
            "Fetching reviews from page 88...\n",
            "Fetching reviews from page 89...\n",
            "Fetching reviews from page 90...\n",
            "Fetching reviews from page 91...\n",
            "Fetching reviews from page 92...\n",
            "Fetching reviews from page 93...\n",
            "Fetching reviews from page 94...\n",
            "Fetching reviews from page 95...\n",
            "Fetching reviews from page 96...\n",
            "Fetching reviews from page 97...\n",
            "Fetching reviews from page 98...\n",
            "Fetching reviews from page 99...\n",
            "Fetching reviews from page 100...\n",
            "Fetching reviews from page 101...\n",
            "Fetching reviews from page 102...\n",
            "Fetching reviews from page 103...\n",
            "Fetching reviews from page 104...\n",
            "Fetching reviews from page 105...\n",
            "Fetching reviews from page 106...\n",
            "Fetching reviews from page 107...\n",
            "Fetching reviews from page 108...\n",
            "Fetching reviews from page 109...\n",
            "Fetching reviews from page 110...\n",
            "Fetching reviews from page 111...\n",
            "Fetching reviews from page 112...\n",
            "Collected 1008 reviews.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBmGtBem_4nf",
        "outputId": "74f08c5d-fc1f-4a8e-8a5c-349ad91b1611"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file with Amazon reviews\n",
        "input_file = 'amazon_reviews_1000.csv'  # Path to your input CSV file\n",
        "output_file = 'cleaned_amazon_reviews.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "reviews_df = pd.read_csv(input_file)\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to clean the text\n",
        "def clean_text(text):\n",
        "    # Step 1: Remove noise (special characters and punctuations)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Step 2: Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Step 3: Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text_tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([word for word in text_tokens if word.lower() not in stop_words])\n",
        "\n",
        "    # Step 4: Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 5: Stemming\n",
        "    text_stemmed = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "    # Step 6: Lemmatization\n",
        "    text_lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    return text_stemmed, text_lemmatized\n",
        "\n",
        "# Apply the cleaning function to the 'Content' column and create new columns\n",
        "reviews_df['Cleaned_Text_Stemmed'], reviews_df['Cleaned_Text_Lemmatized'] = zip(*reviews_df['Content'].apply(clean_text))\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "reviews_df.to_csv(output_file, index=False)\n",
        "\n",
        "# Display the first few rows of the cleaned data\n",
        "print(reviews_df[['Content', 'Cleaned_Text_Stemmed', 'Cleaned_Text_Lemmatized']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hio7DM4_9Dm",
        "outputId": "d0595565-726e-42af-c57a-be7efa192b42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Content  \\\n",
            "0  ⭐️⭐️⭐️⭐️⭐️The SAMSUNG Galaxy S24 Ultra is an a...   \n",
            "1  I had a s20FE which was a very good phone at a...   \n",
            "2  PLEEEEEASE people, stop complaining about the ...   \n",
            "3  The best 1,100 dollars I've spent this year. I...   \n",
            "4  Me encantó. Nuevo, original y en empaque sella...   \n",
            "\n",
            "                                Cleaned_Text_Stemmed  \\\n",
            "0  samsung galaxi ultra absolut powerhous smartph...   \n",
            "1  sfe good phone great price yr batteri would dr...   \n",
            "2  pleeeeeas peopl stop complain display thing li...   \n",
            "3  best dollar ive spent year went note plu two u...   \n",
            "4  encantó nuevo origin en empaqu sellado sin dud...   \n",
            "\n",
            "                             Cleaned_Text_Lemmatized  \n",
            "0  samsung galaxy ultra absolute powerhouse smart...  \n",
            "1  sfe good phone great price yr battery would dr...  \n",
            "2  pleeeeease people stop complaining display thi...  \n",
            "3  best dollar ive spent year went note plus two ...  \n",
            "4  encantó nuevo original en empaque sellado sin ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk spacy\n",
        "!python -m spacy download en_core_web_sm  # Download spaCy's English model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjJkTSoWi7dI",
        "outputId": "e1e6a14f-e680-48d4-c327-1ea27e948fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk.tree import Tree\n",
        "from nltk import pos_tag, word_tokenize, ne_chunk\n",
        "\n",
        "# Load the cleaned data\n",
        "input_file = 'cleaned_amazon_reviews.csv'  # Path to your cleaned CSV file\n",
        "reviews_df = pd.read_csv(input_file)\n",
        "\n",
        "# Load spaCy model for dependency parsing and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download NLTK resources for POS tagging and chunking\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Function to conduct POS tagging\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER)\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Function to perform constituency parsing using NLTK\n",
        "def constituency_parsing(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    chunked = ne_chunk(pos_tags)\n",
        "    return chunked\n",
        "\n",
        "# Function to perform dependency parsing\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    return doc\n",
        "\n",
        "# Analyzing the cleaned text\n",
        "pos_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "entity_counts = Counter()\n",
        "\n",
        "# Loop through each cleaned text entry\n",
        "for text in reviews_df['Cleaned_Text_Lemmatized']:\n",
        "    # POS tagging\n",
        "    pos_tags = pos_tagging(text)\n",
        "    for word, pos in pos_tags:\n",
        "        if pos.startswith('N'):\n",
        "            pos_counts['Noun'] += 1\n",
        "        elif pos.startswith('V'):\n",
        "            pos_counts['Verb'] += 1\n",
        "        elif pos.startswith('J'):\n",
        "            pos_counts['Adjective'] += 1\n",
        "        elif pos.startswith('R'):\n",
        "            pos_counts['Adverb'] += 1\n",
        "\n",
        "    # Named Entity Recognition\n",
        "    entities = extract_entities(text)\n",
        "    entity_counts.update([entity[1] for entity in entities])  # Update the counts for the entity types\n",
        "\n",
        "# Print the total counts of POS tags\n",
        "print(\"Total Parts of Speech Counts:\")\n",
        "print(pos_counts)\n",
        "\n",
        "# Print the entity counts\n",
        "print(\"\\nTotal Named Entity Counts:\")\n",
        "for entity, count in entity_counts.items():\n",
        "    print(f\"{entity}: {count}\")\n",
        "\n",
        "# Example of constituency and dependency parsing\n",
        "example_text = reviews_df['Cleaned_Text_Lemmatized'].iloc[0]  # Example sentence\n",
        "\n",
        "# Constituency Parsing\n",
        "print(\"\\nConstituency Parsing Tree:\")\n",
        "constituency_tree = constituency_parsing(example_text)\n",
        "print(constituency_tree)  # Prints the chunked tree\n",
        "\n",
        "# Dependency Parsing\n",
        "dependency_doc = dependency_parsing(example_text)\n",
        "print(\"\\nDependency Parsing Tree:\")\n",
        "for sent in dependency_doc.sents:\n",
        "    print(sent.root)  # Prints the root of the dependency parse tree\n",
        "    for token in sent:\n",
        "        print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n",
        "\n",
        "# Visualizing the dependency tree\n",
        "# Uncomment the following line if you have Graphviz installed\n",
        "# spacy.displacy.serve(dependency_doc, style=\"dep\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5zf8VcCA9rR",
        "outputId": "4195ca5e-00a3-481b-fc1f-4990bf8a7e8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parts of Speech Counts:\n",
            "{'Noun': 41245, 'Verb': 12785, 'Adjective': 19348, 'Adverb': 7889}\n",
            "\n",
            "Total Named Entity Counts:\n",
            "ORG: 1224\n",
            "PERSON: 1444\n",
            "DATE: 334\n",
            "TIME: 333\n",
            "CARDINAL: 333\n",
            "ORDINAL: 222\n",
            "GPE: 111\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "(S\n",
            "  samsung/NN\n",
            "  galaxy/NN\n",
            "  ultra/JJ\n",
            "  absolute/NN\n",
            "  powerhouse/NN\n",
            "  smartphone/NN\n",
            "  moment/NN\n",
            "  started/VBD\n",
            "  using/VBG\n",
            "  device/NN\n",
            "  clear/JJ\n",
            "  samsung/NN\n",
            "  truly/RB\n",
            "  outdone/JJ\n",
            "  model/NN\n",
            "  gb/JJ\n",
            "  storage/NN\n",
            "  ample/JJ\n",
            "  apps/JJ\n",
            "  photo/NN\n",
            "  video/NN\n",
            "  performance/NN\n",
            "  lightning/VBG\n",
            "  fast/JJ\n",
            "  thanks/NNS\n",
            "  latest/JJS\n",
            "  ai/NN\n",
            "  technology/NN\n",
            "  powerful/JJ\n",
            "  processingthe/JJ\n",
            "  display/NN\n",
            "  simply/RB\n",
            "  stunningvivid/JJ\n",
            "  color/NN\n",
            "  sharp/JJ\n",
            "  detail/NN\n",
            "  deep/JJ\n",
            "  black/JJ\n",
            "  make/NN\n",
            "  everything/NN\n",
            "  streaming/VBG\n",
            "  video/NN\n",
            "  scrolling/VBG\n",
            "  social/JJ\n",
            "  medium/NN\n",
            "  visually/RB\n",
            "  immersive/JJ\n",
            "  experience/NN\n",
            "  build/VBP\n",
            "  quality/NN\n",
            "  topnotch/NN\n",
            "  design/NN\n",
            "  sleek/JJ\n",
            "  premium/NN\n",
            "  giving/VBG\n",
            "  luxurious/JJ\n",
            "  feel/NN\n",
            "  handone/NN\n",
            "  standout/NN\n",
            "  feature/NN\n",
            "  advanced/VBD\n",
            "  camera/NN\n",
            "  system/NN\n",
            "  image/NN\n",
            "  incredibly/RB\n",
            "  detailed/VBD\n",
            "  even/RB\n",
            "  lowlight/JJ\n",
            "  condition/NN\n",
            "  array/NN\n",
            "  shooting/VBG\n",
            "  mode/NN\n",
            "  allows/VBZ\n",
            "  creative/JJ\n",
            "  flexibility/NN\n",
            "  battery/NN\n",
            "  life/NN\n",
            "  impressive/JJ\n",
            "  well/RB\n",
            "  easily/RB\n",
            "  lasting/VBG\n",
            "  full/JJ\n",
            "  day/NN\n",
            "  heavy/JJ\n",
            "  use/NN\n",
            "  without/IN\n",
            "  hitchbeing/VBG\n",
            "  unlocked/JJ\n",
            "  androidbased/JJ\n",
            "  mean/JJ\n",
            "  full/JJ\n",
            "  control/NN\n",
            "  device/NN\n",
            "  switch/NN\n",
            "  carrier/NN\n",
            "  customize/VB\n",
            "  setting/VBG\n",
            "  without/IN\n",
            "  hassle/NNS\n",
            "  software/NN\n",
            "  experience/NN\n",
            "  smooth/JJ\n",
            "  samsungs/NNS\n",
            "  enhancement/NN\n",
            "  adding/VBG\n",
            "  value/NN\n",
            "  without/IN\n",
            "  overwhelming/VBG\n",
            "  core/NN\n",
            "  android/NN\n",
            "  experienceoverall/NN\n",
            "  samsung/NN\n",
            "  galaxy/NN\n",
            "  ultra/JJ\n",
            "  fantastic/JJ\n",
            "  investment/NN\n",
            "  anyone/NN\n",
            "  looking/VBG\n",
            "  highperformance/NN\n",
            "  featurepacked/VBD\n",
            "  smartphone/NN\n",
            "  reliable/JJ\n",
            "  powerful/JJ\n",
            "  offer/NN\n",
            "  exceptional/JJ\n",
            "  user/JJ\n",
            "  experience/NN\n",
            "  highly/RB\n",
            "  recommended/VBD)\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "started\n",
            "samsung --> nsubj --> galaxy\n",
            "galaxy --> nsubj --> started\n",
            "ultra --> amod --> powerhouse\n",
            "absolute --> amod --> powerhouse\n",
            "powerhouse --> dobj --> galaxy\n",
            "smartphone --> compound --> moment\n",
            "moment --> nsubj --> started\n",
            "started --> ROOT --> started\n",
            "using --> xcomp --> started\n",
            "device --> dobj --> using\n",
            "clear --> amod --> samsung\n",
            "samsung --> nmod --> storage\n",
            "truly --> advmod --> outdone\n",
            "outdone --> amod --> storage\n",
            "model --> compound --> storage\n",
            "gb --> compound --> storage\n",
            "storage --> appos --> device\n",
            "ample --> amod --> apps\n",
            "apps --> dobj --> using\n",
            "photo --> compound --> video\n",
            "video --> compound --> performance\n",
            "performance --> compound --> lightning\n",
            "lightning --> dobj --> using\n",
            "fast --> amod --> thanks\n",
            "thanks --> npadvmod --> using\n",
            "latest --> advmod --> ai\n",
            "ai --> dep --> started\n",
            "technology --> nmod --> display\n",
            "powerful --> amod --> display\n",
            "processingthe --> compound --> display\n",
            "display --> pobj --> ai\n",
            "simply --> advmod --> stunningvivid\n",
            "stunningvivid --> npadvmod --> make\n",
            "color --> nmod --> detail\n",
            "sharp --> amod --> detail\n",
            "detail --> dobj --> stunningvivid\n",
            "deep --> amod --> black\n",
            "black --> amod --> detail\n",
            "make --> xcomp --> started\n",
            "everything --> nsubj --> streaming\n",
            "streaming --> ccomp --> make\n",
            "video --> npadvmod --> scrolling\n",
            "scrolling --> advcl --> streaming\n",
            "social --> amod --> medium\n",
            "medium --> dobj --> scrolling\n",
            "visually --> advmod --> immersive\n",
            "immersive --> amod --> experience\n",
            "experience --> nsubj --> build\n",
            "build --> ccomp --> make\n",
            "quality --> compound --> topnotch\n",
            "topnotch --> compound --> premium\n",
            "design --> nmod --> premium\n",
            "sleek --> amod --> premium\n",
            "premium --> dobj --> build\n",
            "giving --> acl --> premium\n",
            "luxurious --> amod --> feel\n",
            "feel --> dobj --> giving\n",
            "handone --> compound --> standout\n",
            "standout --> nsubj --> feature\n",
            "feature --> ccomp --> make\n",
            "advanced --> amod --> image\n",
            "camera --> compound --> system\n",
            "system --> compound --> image\n",
            "image --> dobj --> feature\n",
            "incredibly --> advmod --> detailed\n",
            "detailed --> advcl --> make\n",
            "even --> advmod --> array\n",
            "lowlight --> amod --> condition\n",
            "condition --> compound --> array\n",
            "array --> compound --> mode\n",
            "shooting --> compound --> mode\n",
            "mode --> nsubj --> allows\n",
            "allows --> ccomp --> make\n",
            "creative --> amod --> life\n",
            "flexibility --> compound --> battery\n",
            "battery --> compound --> life\n",
            "life --> nsubj --> impressive\n",
            "impressive --> ccomp --> allows\n",
            "well --> advmod --> lasting\n",
            "easily --> advmod --> lasting\n",
            "lasting --> xcomp --> impressive\n",
            "full --> amod --> day\n",
            "day --> nmod --> use\n",
            "heavy --> amod --> use\n",
            "use --> dobj --> lasting\n",
            "without --> prep --> lasting\n",
            "hitchbeing --> pcomp --> without\n",
            "unlocked --> amod --> setting\n",
            "androidbased --> amod --> mean\n",
            "mean --> amod --> setting\n",
            "full --> amod --> control\n",
            "control --> compound --> device\n",
            "device --> compound --> carrier\n",
            "switch --> compound --> carrier\n",
            "carrier --> compound --> customize\n",
            "customize --> compound --> setting\n",
            "setting --> dobj --> hitchbeing\n",
            "without --> prep --> setting\n",
            "hassle --> compound --> software\n",
            "software --> compound --> experience\n",
            "experience --> pobj --> without\n",
            "smooth --> amod --> samsungs\n",
            "samsungs --> compound --> enhancement\n",
            "enhancement --> npadvmod --> adding\n",
            "adding --> amod --> value\n",
            "value --> dobj --> impressive\n",
            "without --> prep --> impressive\n",
            "overwhelming --> amod --> android\n",
            "core --> compound --> android\n",
            "android --> pobj --> without\n",
            "experienceoverall --> nsubj --> galaxy\n",
            "samsung --> nsubj --> galaxy\n",
            "galaxy --> ccomp --> allows\n",
            "ultra --> amod --> investment\n",
            "fantastic --> amod --> investment\n",
            "investment --> compound --> anyone\n",
            "anyone --> dobj --> galaxy\n",
            "looking --> acl --> anyone\n",
            "highperformance --> nsubj --> featurepacked\n",
            "featurepacked --> ccomp --> allows\n",
            "smartphone --> dobj --> featurepacked\n",
            "reliable --> amod --> experience\n",
            "powerful --> amod --> offer\n",
            "offer --> compound --> experience\n",
            "exceptional --> amod --> experience\n",
            "user --> compound --> experience\n",
            "experience --> nsubj --> recommended\n",
            "highly --> advmod --> recommended\n",
            "recommended --> conj --> started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXNn1lEVbMsv"
      },
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXgGHJnAkP7Y"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://myunt-my.sharepoint.com/:x:/g/personal/sireesharusum_my_unt_edu/EVysDQExkAdKvQ1fN0K5XCwBj7W1vHQFVuEGqsJX5OFddw?e=G9ZNLG"
      ],
      "metadata": {
        "id": "oZXL_IkDBNkN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "outputs": [],
      "source": [
        "# Write your response below\n",
        "Challenges: The assignment's main hurdles were data collecting and cleaning, especially from sites like IMDb and Amazon, where uneven formatting and noise in user-generated material made preprocessing difficult.\n",
        "Enjoyable Aspects: Working with strong NLP tools like spaCy made the job more interesting because it streamlined complicated chores like dependency parsing and POS labelling. The hands-on approach reinforced core NLP ideas, allowing for practical application while strengthening theoretical comprehension.\n",
        "Provided time to complete the assignment is sufficient.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}